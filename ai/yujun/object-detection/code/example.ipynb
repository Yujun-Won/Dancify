{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shortform_generator\n",
    "import sys\n",
    "sys.path.append(\n",
    "    \"/Users/yujunwon/Project/dancify/ai/yujun/object-detection/code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./audio_video/e9dca4c7-fe35-4ae3-acdf-31260cb6bdcf.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "음성 추출이 성공적으로 완료되었습니다.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 사용 예시\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m shortform_generator\u001b[39m.\u001b[39;49mgenerate_video(\n\u001b[1;32m      3\u001b[0m     video_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../code/audio_video/attention.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../code/result/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Project/dancify/ai/yujun/object-detection/code/shortform_generator.py:312\u001b[0m, in \u001b[0;36mgenerate_video\u001b[0;34m(video_path, output_path)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# 2. 키포인트 추출 & 영상 크롭\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m total_frame_no, keypoints_dict \u001b[39m=\u001b[39m calculate_keypoints(video_path)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m total_frame_no \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m keypoints_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m키포인트 추출에 실패하였습니다.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Project/dancify/ai/yujun/object-detection/code/shortform_generator.py:114\u001b[0m, in \u001b[0;36mcalculate_keypoints\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m    112\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m    113\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m results \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:\n\u001b[1;32m    117\u001b[0m     keypoints \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dancify/lib/python3.10/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[1;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dancify/lib/python3.10/site-packages/mediapipe/python/solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[1;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[1;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "shortform_generator.generate_video(\n",
    "    video_path=\"../code/audio_video/attention.mp4\", output_path=\"../code/result/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: 1920, 1080\n",
      "original size: 1080, 1920\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap1 = cv2.VideoCapture(\"./audio_video/attention.mp4\")\n",
    "cap2 = cv2.VideoCapture(\"./audio_video/mola.mp4\")\n",
    "\n",
    "width1 = cap1.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height1 = cap1.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "width2 = cap2.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height2 = cap2.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "print('original size: %d, %d' % (width1, height1))\n",
    "print('original size: %d, %d' % (width2, height2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path):\n",
    "    # VideoCapture 객체 생성\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # 영상의 정보를 얻기 위해 첫 프레임을 읽는다.\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        return\n",
    "\n",
    "    # 영상의 가로 세로 길이를 얻는다.\n",
    "    height, width, _ = frame.shape\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # 9:16 비율을 만들기 위한 새로운 가로 길이를 계산한다.\n",
    "    new_width = int(height * 9 / 16)\n",
    "\n",
    "    # 가운데를 기준으로 이미지를 자른다.\n",
    "    start_x = width // 2 - new_width // 2\n",
    "    end_x = width // 2 + new_width // 2\n",
    "\n",
    "    # 비디오 라이터 설정\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')  # 코덱 지정\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (1080, 1920))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 이미지를 자른다.\n",
    "        cropped = frame[:, start_x:end_x]\n",
    "\n",
    "        # 자른 이미지를 1080x1920으로 보간한다.\n",
    "        resized = cv2.resize(cropped, (1080, 1920), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "        # 보간한 이미지를 쓴다.\n",
    "        out.write(resized)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "resize_video('./result/attention.mp4', './result/attention-interpolated.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dancify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
